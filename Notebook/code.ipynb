{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IImport Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf /content/drive/MyDrive/\"Image Segmentation Workshop\"/IDDSPLIT.tar.gz -C /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/content/IDDCLEAN'\n",
    "train_csv_path = os.path.join(data_path, 'train.csv')\n",
    "val_csv_path = os.path.join(data_path, 'valid.csv')\n",
    "test_csv_path = os.path.join(data_path, 'test.csv')\n",
    "\n",
    "\n",
    "class IDDDataClass(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, mask_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data_frame.iloc[idx, 0])\n",
    "        mask_name = os.path.join(self.mask_dir, self.data_frame.iloc[idx, 1])\n",
    "\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        mask = Image.open(mask_name).convert('RGB')\n",
    "\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        mask = index_mapping(mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = os.path.join(data_path, 'img')\n",
    "train_mask_dir = os.path.join(data_path, 'mask')\n",
    "val_img_dir = os.path.join(data_path, 'img')\n",
    "val_mask_dir = os.path.join(data_path, 'mask')\n",
    "test_img_dir = os.path.join(data_path, 'img')\n",
    "test_mask_dir = os.path.join(data_path, 'mask')\n",
    "\n",
    "train_dataset = IDDDataClass(train_csv_path, train_images_dir, train_mask_dir, transform=transform)\n",
    "val_dataset = IDDDataClass(val_csv_path, val_img_dir, val_mask_dir, transform=transform)\n",
    "test_dataset = IDDDataClass(test_csv_path, test_img_dir, test_mask_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Mapping to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_DICT = {\n",
    "    (128, 64, 128): 0,  # Road\n",
    "    (244, 35, 232): 2,  # Sidewalk\n",
    "    (220, 20, 60): 4,   # Person\n",
    "    (255, 0, 0): 5,     # Rider\n",
    "    (0, 0, 230): 6,     # Motorcycle\n",
    "    (119, 11, 32): 7,   # Bicycle\n",
    "    (0, 0, 142): 9,     # Car\n",
    "    (0, 0, 70): 10,     # Truck\n",
    "    (0, 60, 100): 11,   # Bus\n",
    "    (0, 80, 100): 12,   # Train\n",
    "    (102, 102, 156): 14 # Wall\n",
    "}\n",
    "\n",
    "def index_mapping(mask):\n",
    "    mask = mask.numpy()\n",
    "    index_mask = np.zeros(mask.shape[:2], dtype=np.int64)\n",
    "\n",
    "    for rgb, idx in COLOR_DICT.items():\n",
    "        index_mask[(mask == rgb).all(axis=2)] = idx\n",
    "\n",
    "    return torch.tensor(index_mask, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape, masks.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure the GPU is used if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 11\n",
    "\n",
    "model = fcn_resnet50(pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='/content/logs')\n",
    "\n",
    "def mean_iou(pred, target, n_classes=21):\n",
    "    iou_list = []\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    for cls in range(n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().float()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan'))\n",
    "        else:\n",
    "            iou_list.append(intersection / union)\n",
    "    return torch.mean(torch.tensor(iou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_iou = 0.0\n",
    "\n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_iou += mean_iou(outputs, masks).item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_iou /= len(train_loader)\n",
    "\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('IoU/train', epoch_iou, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Mean IoU: {epoch_iou:.4f}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Visualize the results in TensorBoard\n",
    "print(\"Training complete. Run 'tensorboard --logdir=/content/logs' to visualize the results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(pred, target, n_classes=num_classes):\n",
    "    iou_list = []\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    for cls in range(n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds[target_inds]).sum().float()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan'))\n",
    "        else:\n",
    "            iou_list.append(intersection / union)\n",
    "    return torch.mean(torch.tensor(iou_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, num_classes):\n",
    "    metrics = {\n",
    "        'pixel_accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'iou': [],\n",
    "        'ap': []\n",
    "    }\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(num_classes))\n",
    "    for i in range(num_classes):\n",
    "        TP = cm[i, i]\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        FN = cm[i, :].sum() - TP\n",
    "        TN = cm.sum() - (TP + FP + FN)\n",
    "        \n",
    "        pixel_accuracy = (TP + TN) / cm.sum()\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "        \n",
    "        metrics['pixel_accuracy'].append(pixel_accuracy)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1_score'].append(f1_score)\n",
    "        metrics['iou'].append(iou)\n",
    "        \n",
    "        precision_recall = precision_recall_fscore_support(y_true == i, y_pred == i, average='binary')\n",
    "        metrics['ap'].append(precision_recall[2])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(masks.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0).flatten()\n",
    "    all_labels = np.concatenate(all_labels, axis=0).flatten()\n",
    "    \n",
    "    return calculate_metrics(all_labels, all_preds, num_classes)\n",
    "\n",
    "num_classes = 11\n",
    "test_metrics = evaluate_model(model, test_loader, device, num_classes)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_per_image(pred, target, n_classes=num_classes):\n",
    "    iou_list = []\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    for cls in range(n_classes):\n",
    "        pred_inds = pred == cls\n",
    "        target_inds = target == cls\n",
    "        intersection = (pred_inds & target_inds).sum().float()\n",
    "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
    "        if union == 0:\n",
    "            iou_list.append(float('nan'))\n",
    "        else:\n",
    "            iou_list.append(intersection / union)\n",
    "    return torch.tensor(iou_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_per_image(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_ious = []\n",
    "    all_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(masks.cpu().numpy())\n",
    "            all_images.append(images.cpu().numpy())\n",
    "            for i in range(images.size(0)):\n",
    "                ious = mean_iou_per_image(outputs[i].unsqueeze(0), masks[i].unsqueeze(0)).cpu().numpy()\n",
    "                all_ious.append(ious)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_images = np.concatenate(all_images, axis=0)\n",
    "    all_ious = np.array(all_ious)\n",
    "    \n",
    "    print(\"Evaluation Complete\")\n",
    "    print(\"All Images Shape:\", all_images.shape)\n",
    "    print(\"All Predictions Shape:\", all_preds.shape)\n",
    "    print(\"All Labels Shape:\", all_labels.shape)\n",
    "    print(\"All IOUs Shape:\", all_ious.shape)\n",
    "    \n",
    "    return all_images, all_preds, all_labels, all_ious\n",
    "\n",
    "test_images, test_preds, test_labels, test_ious = evaluate_model_per_image(model, test_loader, device, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_low_iou_images(images, preds, labels, ious, class_idx, num_images=3):\n",
    "    low_iou_indices = np.where(ious[:, class_idx] <= 0.5)[0]\n",
    "    if len(low_iou_indices) < num_images:\n",
    "        num_images = len(low_iou_indices)\n",
    "    selected_indices = np.random.choice(low_iou_indices, num_images, replace=False)\n",
    "    \n",
    "    for idx in selected_indices:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input Image')\n",
    "        plt.imshow(images[idx].transpose(1, 2, 0).astype(np.uint8))\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.imshow(labels[idx])\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Prediction')\n",
    "        plt.imshow(preds[idx])\n",
    "        plt.show()\n",
    "\n",
    "visualize_low_iou_images(test_images, test_preds, test_labels, test_ious, class_idx=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
